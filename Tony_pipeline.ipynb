{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f9ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hmeasure import h_score\n",
    "import matplotlib.pyplot as plt\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE #need to install additional packages for this, otherwise use sklearn.manifold.TSNE\n",
    "from scipy.io import arff\n",
    "from rich import print\n",
    "import joblib\n",
    "import scipy\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "\n",
    "from ks_metric import ks_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "41644521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def preprocessing(data, dataset_name, seed):\n",
    "    data = data.copy()\n",
    "    data.drop_duplicates(inplace=True)\n",
    "\n",
    "    if dataset_name == \"polish\":\n",
    "        # impute \n",
    "        imr = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        imr = imr.fit(data)\n",
    "        data = imr.transform(data)\n",
    "        imputed_data=imr.transform(data)\n",
    "        imputed_data_df=pd.DataFrame(imputed_data)\n",
    "        data=pd.DataFrame(imputed_data_df.values,columns=[\"Attr1\",\"Attr2\",\"Attr3\",\"Attr4\",\"Attr5\",\"Attr6\",\"Attr7\",\"Attr8\",\"Attr9\",\"Attr10\",\"Attr11\",\"Attr12\",\"Attr13\",\"Attr14\",\"Attr15\",\"Attr16\",\"Attr17\",\"Attr18\",\"Attr19\",\"Attr20\",\"Attr21\",\"Attr22\",\"Attr23\",\"Attr24\",\"Attr25\",\"Attr26\",\"Attr27\",\"Attr28\",\"Attr29\",\"Attr30\",\"Attr31\",\"Attr32\",\"Attr33\",\"Attr34\",\"Attr35\",\"Attr36\",\"Attr37\",\"Attr38\",\"Attr39\",\"Attr40\",\"Attr41\",\"Attr42\",\"Attr43\",\"Attr44\",\"Attr45\",\"Attr46\",\"Attr47\",\"Attr48\",\"Attr49\",\"Attr50\",\"Attr51\",\"Attr52\",\"Attr53\",\"Attr54\",\"Attr55\",\"Attr56\",\"Attr57\",\"Attr58\",\"Attr59\",\"Attr60\",\"Attr61\",\"Attr62\",\"Attr63\",\"Attr64\",\"label\"])\n",
    "        # convert class to int\n",
    "        data['label'] = data['label'].astype(int)\n",
    "\n",
    "    if dataset_name == \"norwegian\":\n",
    "        data.pop('v22')\n",
    "        data.pop('year')\n",
    "        data.pop('org_number')\n",
    "\n",
    "    df_train, df_test = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def load_data(dataset, seed):\n",
    "    dataset_name = dataset[\"name\"]\n",
    "    data = pd.read_parquet(f\"../data/{dataset_name}_dataset.parquet\")\n",
    "    if dataset_name == \"polish\":\n",
    "        data.rename(columns={'Class': 'label'}, inplace=True)\n",
    "\n",
    "    # preprocessing\n",
    "    df_train, df_test = preprocessing(data, dataset_name, seed)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def sampling(df_train_raw, sampling_technique, seed):\n",
    "    df_train = df_train_raw.copy()\n",
    "    if sampling_technique == \"RandomUnderSampler\":\n",
    "        rus = RandomUnderSampler(random_state=seed)\n",
    "        X_train, y_train = rus.fit_resample(df_train.drop('label', axis=1), df_train['label'])\n",
    "        df_train = pd.concat([X_train, y_train], axis=1)\n",
    "        return df_train\n",
    "    \n",
    "    elif sampling_technique == \"SMOTE\":\n",
    "        sm = SMOTE(random_state=seed, n_jobs=5)\n",
    "        X_train, y_train = sm.fit_resample(df_train.drop('label', axis=1), df_train['label'])\n",
    "        df_train = pd.concat([X_train, y_train], axis=1)\n",
    "        return df_train\n",
    "\n",
    "    elif sampling_technique == \"Raw\":\n",
    "        return df_train\n",
    "    \n",
    "\n",
    "# transform data\n",
    "def transform_data(df_train, df_test, batch_size = 32):\n",
    "    y_train = df_train.pop('label')\n",
    "    X_train = df_train\n",
    "\n",
    "    y_test = df_test.pop('label')\n",
    "    X_test = df_test\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    y_train = y_train.tolist()\n",
    "    y_test = y_test.tolist()\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    return X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# define VAE architecture\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2_mean = nn.Linear(hidden_size, latent_size)\n",
    "        self.fc2_logvar = nn.Linear(hidden_size, latent_size)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_size, hidden_size)\n",
    "        self.fc4_mean = nn.Linear(hidden_size, input_size)\n",
    "        self.fc4_logvar = nn.Linear(hidden_size, input_size)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h1 = F.tanh(self.fc1(x))\n",
    "        return self.fc2_mean(h1), self.fc2_logvar(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.tanh(self.fc3(z))\n",
    "        return self.fc4_mean(h3), self.fc4_logvar(h3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    \n",
    "def vae_loss(recon_x_mean, recon_x_logvar, x, mu, logvar):\n",
    "    # Reconstruction loss\n",
    "    k = recon_x_mean.size(1)\n",
    "    variance = recon_x_logvar.exp()\n",
    "    logp = (-k / 2.0) * torch.log(2 * torch.tensor(np.pi)) - 0.5 * torch.mean(recon_x_logvar) - torch.mean(0.5 * (1.0 / variance) * torch.square(x - recon_x_mean))\n",
    "    recon_loss = torch.mean(-logp)  # negative log likelihood\n",
    "\n",
    "    # Kullback-Leibler divergence loss\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Total loss\n",
    "    return recon_loss + KLD\n",
    "\n",
    "def train(model, train_loader, optimizer, device, epoch, average_losses):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        recon_mu, recon_logvar = recon_batch\n",
    "        loss = vae_loss(recon_mu, recon_logvar, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    average_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "    average_losses.append(average_loss)\n",
    "\n",
    "    return average_losses\n",
    "\n",
    "def get_latent_space(model, X_train_tensor, X_test_tensor, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_space_train = model.encode(X_train_tensor.to(device))[0].detach().cpu().numpy()\n",
    "        latent_space_test = model.encode(X_test_tensor.to(device))[0].detach().cpu().numpy()\n",
    "    return latent_space_train, latent_space_test\n",
    "\n",
    "\n",
    "def classify(X_train, y_train, X_test, classifier_name, hidden_size, seed):\n",
    "    if classifier_name == \"RandomForest\":\n",
    "        cf = RandomForestClassifier(random_state=seed)\n",
    "        param_grid = {\n",
    "            'n_estimators': [100,300,500],\n",
    "            'max_features': ['auto', 'sqrt','log2'],\n",
    "            'max_depth' : [4,6,8],\n",
    "            'criterion' :['entropy', 'gini'],\n",
    "        }\n",
    "        best_cf = GridSearchCV(cf, param_grid=param_grid, cv=3, verbose=True)\n",
    "    \n",
    "    elif classifier_name == \"XGBoost\":\n",
    "            cf = XGBClassifier(random_state=seed)\n",
    "            param_grid = {\n",
    "                'n_estimators': [100, 300, 500],\n",
    "                'max_depth' : [4, 6, 8],\n",
    "                'learning_rate': [0.1, 0.05, 0.01],\n",
    "            }\n",
    "            best_cf = GridSearchCV(cf, param_grid=param_grid, cv=3, verbose=True)\n",
    "    \n",
    "    elif classifier_name == \"LogisticRegression\":\n",
    "        best_cf = LogisticRegression(random_state=seed, max_iter=500)\n",
    "\n",
    "    elif classifier_name == \"MLP\":\n",
    "        best_cf = MLPClassifier(hidden_layer_sizes=(hidden_size, hidden_size, hidden_size), max_iter=500, alpha=0.0001, random_state=seed, activation='relu', solver='adam',)\n",
    "\n",
    "    # fit model\n",
    "    best_cf.fit(X_train, y_train)\n",
    "\n",
    "    # use best model\n",
    "    if classifier_name == \"RandomForest\" or classifier_name == \"XGBoost\":\n",
    "        best_cf = best_cf.best_estimator_\n",
    "        \n",
    "    # predict on test set\n",
    "    y_pred_prob = best_cf.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "    return best_cf, y_pred, y_pred_prob\n",
    "\n",
    "\n",
    "def calculate_metrics(y_test, y_pred, y_pred_prob):\n",
    "    result = {}\n",
    "    # calculate metrics\n",
    "    result['accuracy_score'] = accuracy_score(y_test, y_pred)\n",
    "    result['precision_score'] = precision_score(y_test, y_pred)\n",
    "    result['recall_score'] = recall_score(y_test, y_pred)\n",
    "    result['f1_score'] = f1_score(y_test, y_pred)\n",
    "    result['auc_score'] = roc_auc_score(y_test, y_pred_prob)\n",
    "    result['h_measure'] = h_score(np.array(y_test), np.array(y_pred_prob))\n",
    "    result['ks_score'] = ks_score(y_test, y_pred_prob)/100\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "77717eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6819, 95)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = []\n",
    "\n",
    "# taiwanse dataset configs\n",
    "taiwanse_dataset = {\"name\":\"taiwanese\",\n",
    "                    \"latent_sizes\": [30,45,60],\n",
    "                    \"hidden_size\": 70,\n",
    "                    }\n",
    "datasets.append(taiwanse_dataset)\n",
    "\n",
    "# norwegian dataset configs\n",
    "norwegian_dataset = {\"name\":\"norwegian\",\n",
    "                    \"latent_sizes\": [20,25,30],\n",
    "                    \"hidden_size\": 45,\n",
    "                    }\n",
    "datasets.append(norwegian_dataset)\n",
    "\n",
    "# polish dataset configs\n",
    "polish_dataset = {\"name\":\"polish\",\n",
    "                  \"latent_sizes\": [20,30,40],\n",
    "                  \"hidden_size\": 50,}\n",
    "datasets.append(polish_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e73d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "seeds = [\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4\n",
    "    ]\n",
    "for seed in seeds: # choose seed\n",
    "    set_seed(seed)\n",
    "    for dataset in datasets: # choose dataset\n",
    "        dataset_name = dataset[\"name\"]\n",
    "        hidden_size = dataset[\"hidden_size\"]\n",
    "        # sampling\n",
    "        sampling_techniques = [\n",
    "            \"SMOTE\",\n",
    "            \"RandomUnderSampler\",\n",
    "            \"Raw\"\n",
    "            ]\n",
    "        for sampling_technique in sampling_techniques: # choose sampling technique\n",
    "            # load data\n",
    "            df_train_raw, df_test = load_data(dataset, seed)\n",
    "            # sampling\n",
    "            df_train_raw = sampling(df_train_raw, sampling_technique, seed)\n",
    "            # transform data\n",
    "            X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_train, y_train, X_test, y_test = transform_data(df_train_raw, df_test)\n",
    "\n",
    "            # create dataloader\n",
    "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)    \n",
    "        \n",
    "            # get latent sizes\n",
    "            latent_sizes = dataset[\"latent_sizes\"]\n",
    "            for latent_size in latent_sizes: # choose latent size\n",
    "                # train VAE\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                model = VAE(input_size=X_train.shape[1], hidden_size=hidden_size, latent_size=latent_size).to(device)\n",
    "                optimizer = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "                epochs = 50\n",
    "                average_losses = []\n",
    "                for epoch in range(1, epochs + 1):\n",
    "                    average_losses = train(model, train_loader, optimizer, device, epoch, average_losses)\n",
    "                \n",
    "                # save model\n",
    "                torch.save(model.state_dict(), f\"models/{dataset_name}_{sampling_technique}_{latent_size}_{seed}.pt\")\n",
    "\n",
    "                # get latent representation\n",
    "                latent_space_train, latent_space_test = get_latent_space(model, X_train_tensor, X_test_tensor, device)\n",
    "                # TSNE transformation\n",
    "                tsne = TSNE(n_components=2, random_state=42, n_jobs=5)\n",
    "                latent_space_train_tsne = tsne.fit_transform(latent_space_train)\n",
    "                latent_space_test_tsne = tsne.fit_transform(latent_space_test)\n",
    "\n",
    "                # classification\n",
    "                classifiers = [\"LogisticRegression\", \"RandomForest\", \"XGBoost\", \"MLP\"]\n",
    "                for classifier in classifiers:\n",
    "                    # classification using 'raw' data\n",
    "                    best_cf, y_pred, y_pred_prob = classify(X_train, y_train, X_test, classifier, hidden_size, seed)\n",
    "                    # save model\n",
    "                    joblib.dump(best_cf, f\"models/{dataset_name}_{sampling_technique}_{latent_size}_{classifier}_{seed}.pkl\")\n",
    "                    # make and save histogram of predictions\n",
    "                    plt.hist(y_pred_prob, bins=20)\n",
    "                    plt.savefig(f\"plots/{dataset_name}_{sampling_technique}_{latent_size}_{classifier}_{seed}.png\")\n",
    "                    plt.close()\n",
    "\n",
    "                    # calculate metrics\n",
    "                    result = calculate_metrics(y_test, y_pred, y_pred_prob)\n",
    "                    config = {\n",
    "                        'seed': seed,\n",
    "                        'dataset': dataset_name,\n",
    "                        'sampling_technique': sampling_technique,\n",
    "                        'latent_size': latent_size,\n",
    "                        'hidden_size': hidden_size,\n",
    "                        'classifier': classifier,\n",
    "                    }\n",
    "                    output_data = {\n",
    "                        'y_pred': y_pred,\n",
    "                        'y_pred_prob': y_pred_prob,\n",
    "                        'y_test': np.array(y_test),\n",
    "                    }\n",
    "                    # append results\n",
    "                    results.append({\n",
    "                        'config': config,\n",
    "                        'metrics': result,\n",
    "                        'data': output_data,\n",
    "                    })\n",
    "    \n",
    "                    # classification using latent space\n",
    "                    best_cf, y_pred, y_pred_prob = classify(latent_space_train, y_train, latent_space_test, classifier, hidden_size, seed)\n",
    "                    # save model\n",
    "                    joblib.dump(best_cf, f\"models/{dataset_name}_{sampling_technique}_{latent_size}_{classifier}_{seed}.pkl\")\n",
    "                    # make and save histogram of predictions\n",
    "                    plt.hist(y_pred_prob, bins=20)\n",
    "                    plt.savefig(f\"plots/{dataset_name}_{sampling_technique}_{latent_size}_{classifier}_{seed}.png\")\n",
    "                    plt.close()\n",
    "\n",
    "                    # calculate metrics\n",
    "                    result = calculate_metrics(y_test, y_pred, y_pred_prob)\n",
    "                    config = {\n",
    "                        'seed': seed,\n",
    "                        'dataset': dataset_name,\n",
    "                        'sampling_technique': sampling_technique,\n",
    "                        'latent_size': latent_size,\n",
    "                        'hidden_size': hidden_size,\n",
    "                        'classifier': classifier,\n",
    "                    }\n",
    "                    output_data = {\n",
    "                        'y_pred': y_pred,\n",
    "                        'y_pred_prob': y_pred_prob,\n",
    "                        'y_test': y_test,\n",
    "                    }\n",
    "                    vae_data = {\n",
    "                        'latent_space_train': latent_space_train,\n",
    "                        'latent_space_test': latent_space_test,\n",
    "                        'average_losses': average_losses,\n",
    "                    }\n",
    "                    # append results\n",
    "                    results.append({\n",
    "                        'config': config,\n",
    "                        'metrics': result,\n",
    "                        'data': output_data,\n",
    "                        'latent_space_data': vae_data,\n",
    "                    })\n",
    "\n",
    "                    # plot latent representations using y_test as color\n",
    "                    plt.scatter(latent_space_test_tsne[:, 0], latent_space_test_tsne[:, 1], c=y_test, cmap='viridis',s=5)\n",
    "                    plt.xlabel('t-SNE Dimension 1')\n",
    "                    plt.ylabel('t-SNE Dimension 2')\n",
    "                    plt.title(f'{dataset_name} - {sampling_technique} - {latent_size} - {classifier} - {seed}_true')\n",
    "                    plt.savefig(f\"plots/{dataset_name}_{sampling_technique}_{latent_size}_{classifier}_{seed}_true.png\")\n",
    "                    plt.close()\n",
    "                    \n",
    "                    # plot latent representations using y_pred_prob as color\n",
    "                    plt.scatter(latent_space_test_tsne[:, 0], latent_space_test_tsne[:, 1], c=y_pred_prob, cmap='viridis',s=5)\n",
    "                    plt.xlabel('t-SNE Dimension 1')\n",
    "                    plt.ylabel('t-SNE Dimension 2')\n",
    "                    plt.title(f'{dataset_name} - {sampling_technique} - {latent_size} - {classifier} - {seed}_pred')\n",
    "                    plt.savefig(f\"plots/{dataset_name}_{sampling_technique}_{latent_size}_{classifier}_{seed}_pred.png\")\n",
    "                    plt.close()\n",
    "\n",
    "                    # print confirmation message\n",
    "                    print(f\"{dataset_name} - {sampling_technique} - {latent_size} - {classifier} - {seed} - DONE\")    \n",
    "\n",
    "# convert numpy arrays to lists\n",
    "for result in results:\n",
    "    # check if array\n",
    "    if isinstance(result['data']['y_pred'], np.ndarray):\n",
    "        result['data']['y_pred'] = result['data']['y_pred'].tolist()\n",
    "    if isinstance(result['data']['y_pred_prob'], np.ndarray):\n",
    "        result['data']['y_pred_prob'] = result['data']['y_pred_prob'].tolist()\n",
    "    if isinstance(result['data']['y_test'], np.ndarray):\n",
    "        result['data']['y_test'] = result['data']['y_test'].tolist()     \n",
    "    if isinstance(result['latent_space_data']['latent_space_train'], np.ndarray):\n",
    "        result['latent_space_data']['latent_space_train'] = result['latent_space_data']['latent_space_train'].tolist()\n",
    "    if isinstance(result['latent_space_data']['latent_space_test'], np.ndarray):\n",
    "        result['latent_space_data']['latent_space_test'] = result['latent_space_data']['latent_space_test'].tolist()           \n",
    "\n",
    "# save results\n",
    "with open('results/results.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a67e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/results.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# select results for norwegian dataset\n",
    "results = []\n",
    "for result in data:\n",
    "    if result['config']['dataset'] == 'norwegian':\n",
    "        results.append(result)\n",
    "\n",
    "\n",
    "def calculate_average_metric(results, metric, latent_size, classifier, sampling_technique,vae = False):\n",
    "    data = []\n",
    "    if vae:\n",
    "        for result in results:\n",
    "            if 'latent_space_data' in result.keys():\n",
    "                if result['config']['classifier'] == classifier and result['config']['sampling_technique'] == sampling_technique and result['config']['latent_size'] == latent_size:\n",
    "                    data.append(result['metrics'][f'{metric}'])\n",
    "    else:\n",
    "        for result in results:\n",
    "            if 'latent_space_data' not in result.keys():\n",
    "                if result['config']['classifier'] == classifier and result['config']['sampling_technique'] == sampling_technique:\n",
    "                    data.append(result['metrics'][f'{metric}'])\n",
    "\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    upper = mean + std\n",
    "    lower = mean - std\n",
    "\n",
    "    return round(mean, 4), round(lower,4), round(upper,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb78b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate average metrics for train embeddings\n",
    "for i in [20,25,30]: # adjust latent size depending on the current dataset\n",
    "    print(calculate_average_metric(results,\n",
    "    'ks_score', # Metric to calculate\n",
    "    i,\n",
    "    'LogisticRegression', # Classifier \n",
    "    'SMOTE', # Sampling technique\n",
    "    vae = True\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate average metrics for raw train data\n",
    "for i in ['LogisticRegression','RandomForest','XGBoost','MLP']:\n",
    "    print(calculate_average_metric(results,\n",
    "    'ks_score', # Metric to calculate\n",
    "    '', # leave empty\n",
    "    i, # Classifier\n",
    "    'SMOTE', # Sampling technique\n",
    "    vae = False\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
